---
title: "Denoise simulated length variants with noise results"
author: "Hannah Swan"
date: "2025-01-09"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


First, begin by loading necessary functions that we've written plus packages that we might need to analyze results 
```{r, echo=FALSE}
source("/scratch/hswan/thesis_isomiR_count_denoising/denoise_isomiR_counts_WORKING_FUNCTION.R")
source("/scratch/hswan/thesis_isomiR_count_denoising/load_mouse_miRNA_data_function.R")
library(tidyverse)
library(DESeq2)
transition_probs = readRDS("/scratch/hswan/thesis_isomiR_count_denoising/initial_transition_probs.Rds")
```

### Description of simulations that generated the length variant isomiR counts 

\n

We began with Ernesto's benchmark dataset with contains isomiR-level expression data from 2 different mouse tissues combined in various ratios. After aligning to miRGeneDB reference genome using sRNAbench there are 1007 unique "miRNAs" present in the dataset. However, some isomiR sequences map to multiple miRNAs. Depending on the combination of miRNAs a given read maps to, this might generate a new "miRNA" name that is not a true miRNA. Therefore, we considered sequences mapping only to true miRNAs in the dataset. After filtering, this left us with 759 miRNAs. 
```{r}
mousedata = load_mouse_miRNA_data()
```

```{r}
cat("mousedata object contains the following items:", names(mousedata), "\n")
rowdata = mousedata$rowdata
countdata = mousedata$countdata

cat("rowdata dimensions", dim(rowdata), "\n")
cat("countdata dimensions", dim(countdata), "\n")

cat("Number of unique miRNAs pre-filtering:", length(unique(rowdata$miRNA_name)), "\n")

true_miRNAs = get_true_mouse_miRNAs(rowdata)
cat("Number of true unique miRNAs post-filtering:", length(true_miRNAs), "\n")
```

For each miRNA, we identify an initial center sequence according to our definition. We sum the reads for each sequence across all samples. Then, for a given miRNA we identify the center sequence as the sequence mapping to the given miRNA with the greatest observed read count. From the center sequence we then generate length variant sequences. For example, consider the following miRNA with identified center sequence:
```{r, echo=FALSE}
mirna = true_miRNAs[1]
cat("miRNA:", mirna, "\n")

counts = rowSums(countdata)

count_df = cbind(rowdata, count=counts) %>% data.frame()
center_seq = filter(count_df, miRNA_name == mirna) %>% filter(., count==max(count)) %>% select(., uniqueSequence) %>% unlist() %>% unname()
cat("Center sequence:", center_seq, "\n")
```

First, we can sample the number of differences between the isomiR sequence and the center sequence we've identified from a uniform distribution. We let $1$ be the minimum number of differences between the center sequence and the isomiR sequence and $7$ be the largest number of differences. Frankly, $7$ is probably too big and we would consider something with $7$ nucleotides missing to just be a fragment or throw it out entirely. 
```{r}
set.seed(1989)
num_differences = sample(1:7, 1) 
cat("Number of differences:", num_differences, "\n")
```

The differences in length can occur at either the 5p end or the 3p end. For each removal of a nucleotide, we sample the end it will occur at by flipping a coin. 
```{r}

set.seed(13)
for(i in 1:num_differences){
  x = sample(c("3p", "5p"), 1)
  cat("end:",x, "\n")
}

```
First, we draw 5p. That means we will remove the first nucleotide at the 5p end of the center sequence for our miRNA. Next, we draw 3p. That means the second nucleotide we are removing to create a length variant will be removed from the 3p end of the center sequence. This then generates the length variant isomiR sequence that will appear in our simulated dataset for the given miRNA: 
```{r}
seq = center_seq
set.seed(13)
for(i in 1:num_differences){
  x = sample(c("3p", "5p"), 1)
  if(x == "3p"){
    seq = substr(seq, start=1, stop=nchar(seq)-1)
  } else if(x=="5p"){
    seq = substr(seq, start = 2, stop=nchar(seq))
  }
}

Biostrings::pairwiseAlignment(pattern=center_seq, seq)
```

We can repeat this process of sampling differences and the ends from which we remove the nucleotides to generate those differences to ultimately generate a set of length variant isomiR sequences. Then, the next step is simulating read counts for these isomiR sequences so we have a full simulated datatset that we can pass to our algorithm. Under our error model, we assume that read counts of technical isomiR sequences follow a Poisson distribution with parameter $\mu = n_j \lambda_{ji}$, where $n_j$ is the observed read count of the center sequence and $\lambda_{ji}$ is the probability of generating a read of sequence $i$ from sequence $j$. We calculate this $lambda_{ji}$ by aligning sequence $i$ to center sequence $j$, getting the individual transitions, and taking the product of the individual transition probabilities.
```{r}
align = Biostrings::pairwiseAlignment(pattern = center_seq, seq)
transition = get_transitions(align)
lambda = compute_lambda(transition, transition_probs)
cat("Lambda:", lambda, "\n")
nj = filter(count_df, uniqueSequence == center_seq) %>% select(., count) %>% unlist() %>% unname()
cat("nj:", nj, "\n")
cat("nj times lambda:", nj*lambda, "\n")
mean = nj*lambda
```


Finally, we draw a read count for the length variant isomiR sequence:
```{r}
set.seed(0208)
y = rpois(1, mean)
cat("read count:", y, "\n")
```
We can also add a bit of noise to the draw by sampling noise from a uniform distribution. I made the decision to use the uniform distribution and only addition so that I make sure I have draws within the domain of the Poisson distribution for the denoising algorithm. We know that the mean and the variance of a Poisson distribution are equal. I draw a number between 0 and the standard deviation of the error distribution and add it to the read count we drew.
```{r}
set.seed(1997)
noise = sample(0:sqrt(mean), 1)
cat("Noise:", noise, "\n")
y_w_noise = y + noise
cat("read count with added noise:", y_w_noise, "\n")
```

In this way ,we can generate a simulated read count with or without additional noise from the error distribution from each of the length variant isomiR sequences we generated. This allows us to put together a fully simulated dataset of length variant isomiR sequences + read counts we simulated according to our assumed error model with added noise. We can generate as many datasets as we want for each miRNA. 
```{r, echo = FALSE}
no_noise_data_files = list.files("/scratch/hswan/thesis_isomiR_count_denoising/data/simulated_data/length_variants_NO_NOISE")
data_file_path = "/scratch/hswan/thesis_isomiR_count_denoising/data/simulated_data/length_variants_NO_NOISE"
sample_datasets = paste0(data_file_path, "/", no_noise_data_files[1]) %>% readRDS()

cat("Number of datasets:", length(sample_datasets), "\n")
cat("Each element of sample dataset list contains:", names(sample_datasets[[1]]), "\n")
cat("Row data to be used in isomiR count denoising:\n")
sim_rowdata = sample_datasets[[1]]$rowdata
head(sim_rowdata)
cat("Number of sequences in simulated rowdata:", nrow(sim_rowdata), "\n")
sim_counts = sample_datasets[[1]]$counts
cat("Count data to be denoised via isomiR count denoising algorithm\n")
head(sim_counts)
```

For each simulated dataset, since we simulated directly from the error distribution with some added noise, we should create only 1 partition of isomiR sequences. The center sequence should be the center sequence we manipulated to generate the isomiR sequences and associated read counts. Any additional sequences identified as center sequences are false positives. Therefore, we can calculate the false positive rate of a given dataset by dividing the number of partitions created - 1 by the number of sequences in the dataset - 1: $FPR = \frac{\text{num_partitions} - 1}{\text{Number of sequences}}.$ 
##### Example:
```{r}
example_denoise = denoise_isomiR_counts(sim_rowdata, sim_counts, transition_probs, miRNA = sim_rowdata$miRNA_name[1], 0.05, 10, "BH")

example_partition_df = example_denoise$partition_df

cat("Number of partitions created:", max(example_partition_df$partition), "\n")
cat("Number of false positives:", max(example_partition_df$partition)-1, "\n")

```

We repeatedly applied the denoising algorithm to all 100 of the datasets we generated for this miRNA and have the false positive rate, true negative rate, etc. saved as well as the resulting partition objects. 
```{r}
sample_results_files = list.files("/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/results")
sample_results = paste0("/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/results/", sample_results_files[1]) %>% readRDS()
cat("Objects saved in results object:", names(sample_results), "\n")
tibble(sample_results)
cat("\n tibble of sample results object:\n")
sample_results_w_noise_files = list.files("/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/results")
sample_results_w_noise = paste0("/scratch/hswan/thesis_isomiR_count_denoising/sims/01_09_2025/results/", "Mmu-Mir-21_5p_simulated_length_variant_results.Rds") %>% readRDS()
cat("\n tibble of sample_results_w_noise object:\n")
tibble(sample_results_w_noise)

```
For a given miRNA, we can get the number of false positives for each dataset as well as calculate the false positive rate of identifying true isomiR sequences for that miRNA 
```{r, echo = FALSE}
mirna = strsplit(sample_results_files[1], "_simulated")[[1]][1]
cat("miRNA:", mirna, "\n")
num_fp = sample_results$num_false_positives
cat("Number of false positives in each dataset:", num_fp, "\n")
fp_rate = num_fp/(num_fp + sample_results$num_true_negatives)

#make data frame so we can plot it 
false_positive_df=data.frame(x=1:100, y=fp_rate)
p = ggplot(false_positive_df, aes(x=x, y=y, color='obs')) + geom_point() + xlab("Dataset index") + ylab("False positive rate") + ggtitle(paste0("False positive rate in each dataset for miRNA ", mirna)) + scale_x_continuous(limits=c(0,100)) + scale_y_continuous(limits=c(-0.05, 0.05)) + geom_smooth(aes(x, x*0+0.05, color="nominal")) + scale_color_manual(breaks=c("obs", "nominal"), values=c("obs"="black", "nominal"="red"))
ggsave(paste0("/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/plots/", mirna, "fp_plot.png", collapse=""), p)
```


```{r}
#function that takes the results file name and plots the false positive rate of each dataset vs the dataset index so we can see which simulated datasets, if any, have a false positive rate of creating isomiR partitions that is as extreme or more extreme than the nominal false positive rate (0.05)
#function saves the plot that we create to plots folder, will also print the plot we create if print_plt argument is set to TRUE. Default is FALSE so that we don't print out like 700 plots. 
plt_fp_rate = function(result_file_name, print_plt = FALSE){
  mirna = strsplit(result_file_name, "_simulated")[[1]][1]
  file_path = "/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/results/"
  results = readRDS(paste0(file_path, result_file_name, collapse=""))
  num_fp = results$num_false_positives
  fp_rate = num_fp/(num_fp + results$num_true_negatives)
  false_positive_df = data.frame(x=1:length(fp_rate), y=fp_rate)
  p = suppressWarnings(ggplot(false_positive_df, aes(x=x, y=y, color='obs')) + geom_point() + xlab("Dataset index") + ylab("False positive rate") + ggtitle(paste0("False positive rate in each dataset for miRNA ", mirna)) + scale_x_continuous(limits=c(0,100)) + scale_y_continuous(limits=c(-0.05, 0.05)) + geom_smooth(aes(x, x*0+0.05, color="nominal")) + scale_color_manual(breaks=c("obs", "nominal"), values=c("obs"="black", "nominal"="red")))
  cat("Saving plot to", paste0("/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/plots/", mirna, "_fp_plot.png", collapse=""), "\n")
  ggsave(paste0("/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/plots/", mirna, "fp_plot.png", collapse=""), p)
  if(print_plt){
    print(p)
  }
}

```


```{r}
for(i in 1:5){
  plt_fp_rate(sample_results_files[i], TRUE)
}
cat("Generating separate plots for each miRNA and saving \n")
if(length(list.files("/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/plots")) == 0){
  sapply(sample_results_files, plt_fp_rate)
}

```


Getting results from correctly simulated no noise data containing only length variants that are shorter than the center sequence 
```{r, echo = FALSE}
results_file_path = "/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/results"
results_files = list.files(results_file_path)
results = lapply(results_files, function(x) return(paste0(results_file_path, "/", x) %>% readRDS()))


mirnas = sapply(results_files, function(x) strsplit(x, "_simulated")[[1]][1])
#first extract the number of partitions created for each miRNA:
num_partitions_by_mirna = lapply(results, function(x) return(x[['num_partitions_created']])) %>% unlist()
miRNA_name_col = vector()
for(i in 1:length(mirnas)){
  miRNA_name_col = c(miRNA_name_col, rep(mirnas[i], 100))
}
num_partitions_df = data.frame('num_partitions' = num_partitions_by_mirna, 'miRNA_name' = miRNA_name_col)
num_partitions_df$miRNA_name = as.factor(num_partitions_df$miRNA_name)
num_partitions_df$dataset_idx = rep(1:100, length(mirnas))
ggplot(num_partitions_df, aes(x=dataset_idx, y=num_partitions, color = miRNA_name)) + geom_point() + xlab("Dataset index") + ylab("Number of partitions created in denoising dataset counts") + ggtitle("Number of partitions vs. dataset index in simulated data")+theme(legend.position = "None")
cat("Average number of partitions created:", mean(num_partitions_df$num_partitions), "\n")
```

```{r, echo = FALSE}
num_false_positives_by_mirna = lapply(results, function(x) return(x[["num_false_positives"]])) %>% unlist()
num_false_positives_df = data.frame('num_fp' = num_false_positives_by_mirna)
num_false_positives_df$miRNA_name = num_partitions_df$miRNA_name
num_false_positives_df$dataset_idx = num_partitions_df$dataset_idx


ggplot(num_false_positives_df, aes(x=dataset_idx, y=num_fp, color = miRNA_name)) + geom_point() + theme(legend.position="None") + xlab("Dataset idx") + ylab("Number of false positives") + ggtitle("Number of false positives vs. dataset idx across all miRNAs")
cat("Average number of false positives across all miRNAs and all simulated datasets:", mean(num_false_positives_by_mirna), "\n")
cat("Maxmimum number of false positives across all miRNAs and all simulated datasets:", max(num_false_positives_by_mirna), "\n")
```

Key takeaways: Our algorithm definitely seems to be acting as we would expect. We're well below the nominal false positive rate of $\alpha=0.05$. We are generating draws probablistically so its reasonable that on occasion we draw a simulated count from a low density part of the distribution that gets picked as a true sequence even though its not. 
\n 

Next, I'm going to look at the sequences that we've simulated along with their read counts to see which ones are getting picked out as true sequences. I suspect that they are sequences with a "larger" degree of difference from the center sequence, i.e. more deletions. The "farther" a sequence is from the center sequence i.e. the more changes that need to happen to the center sequence to generate the observed isomiR sequence, the less likely it is to generate this sequence from just technical effects in the sequencing process. Therefore, if we observe just 1 read count of this sequence, we are likely to give it its own partition. I don't think this is necessarily good or bad right now, I think there's merits both to allowing this sequence to form its own partition and to keeping it from forming its own partition. 

```{r}
#get miRNAs with simulated datasets that have false positives:
mirnas_w_fp = filter(num_false_positives_df, num_fp > 0) %>% select(., miRNA_name) %>% unname() %>% unlist() %>% as.character() %>% unique()
#get the datasets for one of the miRNAs we want:
miRNA = mirnas_w_fp[4]
cat("miRNA:",miRNA, "\n")
partition_obj_path = '/scratch/hswan/thesis_isomiR_count_denoising/sims/01_14_2025/NO_NOISE/partition_objs/'
partition_obj_files = list.files(partition_obj_path)
filename = partition_obj_files[grepl(miRNA, partition_obj_files)]
partition_objs = paste0(partition_obj_path, filename, collapse="") %>% readRDS()

dataset_idx =
  filter(num_false_positives_df, num_fp > 0 & miRNA_name==miRNA) %>% select(., dataset_idx) %>% unlist() %>% unname()
print(dataset_idx)
p = partition_objs[[dataset_idx[1]]]
p$partition_df %>% filter(., center == 1) 

fp_seq = p$partition_df %>% filter(., center == 1 & partition != 1) %>% select(., uniqueSequence) %>% unlist() %>% unname()
cat("False positive seq:", fp_seq, "\n")
p$alignments[[fp_seq]]
#alignment looks correct 

lambda_fp_seq = p$master_lambdas$`1`[[fp_seq]]

mu = lambda_fp_seq * p$partition_df$count[p$partition_df$partition==1 & p$partition_df$center==1]
num = ppois(5-1, mu, lower.tail=F)
print(num)
denom = 1-dpois(0, mu)
print(denom)
cat("Raw p-value:", num/denom, "\n")
#looks like we did this correctly and it just happened that we drew a large count by chance 
```

